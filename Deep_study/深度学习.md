# 早期模型
**感知机**：Rosenblatt [1958]最早提出可以模拟人类感知能力的神经网络模型，并称之为感知器（Perceptron），并提出了一种接近于人类学习过程（迭代、试错）的学习算法
**前馈神经网络**
1. 是一种最简单的神经网络，各神经元分层排列。每个神经元只与前一层的神经元相连。接收前一层的输出，并输出给下一层．各层间没有反馈。是目前应用最广泛、发展最迅速的人工神经网络之一。研究从20世纪60年代开始，目前理论研究和实际应用达到了很高的水平。
2. 只允许信号从输入到输出以一种方式传播，没有反馈，是最简单的人工神经网络。基于可能存在的隐藏层，又可以进一步分类为单层或多层前馈神经网络.

**hopfield**：是一种循环神经网络，从输出到输入都有反馈连接。在输入的激励下，会产生不断的状态变化
![[Hopfield网络结构图.png]]
主要用途在于存储信息和TSP问题求解(寻找最短路线)

**自组织特征映射网络SOFM**又称自组织映射网络[SOM](https://so.csdn.net/so/search?q=SOM&spm=1001.2101.3001.7020)，是一种自组织竞争神经网络，一个神经网络接受外界输入模式时，将会分为不同的对应区域，各区域对输入模式具有不同的响应特征，而且这个过程是自动完成的。其特点与人脑的自组织特性相类似。
（竞争性学习：）

# 基础
## 概念
### 基础结构 
#### 1.神经元[网站](https://www.cnblogs.com/subconscious/p/5058741.html)
人工神经网络是由一个个神经元组成的，神经元是人工神经网络中最基础的计算单元。
神经元模型是一个包含输入，输出与计算功能的模型
#### 2.输入层
##### 3.隐藏层（中间层）
根据经验预先设定几个可选值，通过切换这几个值来看整个模型的预测效果，选择效果最好的值作为最终选择。这种方法又叫做Grid Search（网格搜索）。
1. (参数数量可以相同时候，增加层数，即更深的层次表达，好处是更深入的表示特征，以及更强的函数模拟能力)
2. (**更深入的表示特征**可以这样理解，随着网络的层数增加，每一层对于前一层次的抽象表示更深入。在神经网络中，每一层神经元学习到的是前一层神经元值的更抽象的表示。例如第一个隐藏层学习到的是“边缘”的特征，第二个隐藏层学习到的是由“边缘”组成的“形状”的特征，第三个隐藏层学习到的是由“形状”组成的“图案”的特征，最后的隐藏层学习到的是由“图案”组成的“目标”的特征。通过抽取更抽象的特征来对事物进行区分，从而获得更好的区分与分类能力。)
3. (更强的函数模拟能力是由于随着层数的增加，整个网络的参数就越多。而神经网络其实本质就是模拟特征与目标之间的真实关系函数的方法，更多的参数意味着其模拟的函数可以更加的复杂，可以有更多的**容量**（capcity）去拟合真正的关系。)
(？？？中间层我怎么知道要设置多少层数???)
#### 4.输出层
![[单层神经网络.jpg]]

例如，输入的变量是[a1，a2，a3]T（代表由a1，a2，a3组成的列向量），用向量**a**来表示。方程的左边是[z1，z2]T，用向量**z**来表示。系数则是矩阵**W**（2行3列的矩阵，排列形式与公式中的一样)。于是，输出公式可以改写成：g(**W** * **a**) = **z**;这个公式就是神经网络中从前一层计算后一层的矩阵运算

![[多层神经网络表示能力.jpg]]

可以看出，随着层数增加，其非线性分界拟合能力不断增强。图中的分界线并不代表真实训练出的效果，更多的是示意效果。

### 激活函数
sigmoid激活函数
双曲正切函数
ReLU
Leaky
Maxout激活函数