# 早期模型
**感知机**：Rosenblatt [1958]最早提出可以模拟人类感知能力的神经网络模型，并称之为感知器（Perceptron），并提出了一种接近于人类学习过程（迭代、试错）的学习算法
**前馈神经网络**
1. 是一种最简单的神经网络，各神经元分层排列。每个神经元只与前一层的神经元相连。接收前一层的输出，并输出给下一层．各层间没有反馈。是目前应用最广泛、发展最迅速的人工神经网络之一。研究从20世纪60年代开始，目前理论研究和实际应用达到了很高的水平。
2. 只允许信号从输入到输出以一种方式传播，没有反馈，是最简单的人工神经网络。基于可能存在的隐藏层，又可以进一步分类为单层或多层前馈神经网络.

**hopfield**：是一种循环神经网络，从输出到输入都有反馈连接。在输入的激励下，会产生不断的状态变化
![[Hopfield网络结构图.png]]
主要用途在于存储信息和TSP问题求解(寻找最短路线)

**自组织特征映射网络SOFM**又称自组织映射网络[SOM](https://so.csdn.net/so/search?q=SOM&spm=1001.2101.3001.7020)，是一种自组织竞争神经网络，一个神经网络接受外界输入模式时，将会分为不同的对应区域，各区域对输入模式具有不同的响应特征，而且这个过程是自动完成的。其特点与人脑的自组织特性相类似。
（竞争性学习：）

# 基础
## 概念
### 基础结构 
#### 1.神经元[网站](https://www.cnblogs.com/subconscious/p/5058741.html)
人工神经网络是由一个个神经元组成的，神经元是人工神经网络中最基础的计算单元。
神经元模型是一个包含输入，输出与计算功能的模型
#### 2.输入层
##### 3.隐藏层（中间层）
根据经验预先设定几个可选值，通过切换这几个值来看整个模型的预测效果，选择效果最好的值作为最终选择。这种方法又叫做Grid Search（网格搜索）。
1. (参数数量可以相同时候，增加层数，即更深的层次表达，好处是更深入的表示特征，以及更强的函数模拟能力)
2. (**更深入的表示特征**可以这样理解，随着网络的层数增加，每一层对于前一层次的抽象表示更深入。在神经网络中，每一层神经元学习到的是前一层神经元值的更抽象的表示。例如第一个隐藏层学习到的是“边缘”的特征，第二个隐藏层学习到的是由“边缘”组成的“形状”的特征，第三个隐藏层学习到的是由“形状”组成的“图案”的特征，最后的隐藏层学习到的是由“图案”组成的“目标”的特征。通过抽取更抽象的特征来对事物进行区分，从而获得更好的区分与分类能力。)
3. (更强的函数模拟能力是由于随着层数的增加，整个网络的参数就越多。而神经网络其实本质就是模拟特征与目标之间的真实关系函数的方法，更多的参数意味着其模拟的函数可以更加的复杂，可以有更多的**容量**（capcity）去拟合真正的关系。)
(？？？中间层我怎么知道要设置多少层数???)
(输出层我们是以什么形式进行输出？是呈现数据可视化吗？比如是折线图或比例图？)
(调试时候，我们输出的数据是不是要以矩阵的形式输出，可是这样的话一般不是很多数吗？较难判别)
#### 4.输出层
![[单层神经网络.jpg]]

例如，输入的变量是[a1，a2，a3]T（代表由a1，a2，a3组成的列向量），用向量**a**来表示。方程的左边是[z1，z2]T，用向量**z**来表示。系数则是矩阵**W**（2行3列的矩阵，排列形式与公式中的一样)。于是，输出公式可以改写成：g(**W** * **a**) = **z**;这个公式就是神经网络中从前一层计算后一层的矩阵运算

![[多层神经网络表示能力.jpg]]

可以看出，随着层数增加，其非线性分界拟合能力不断增强。图中的分界线并不代表真实训练出的效果，更多的是示意效果。

### 激活函数
* 定义：在多层神经网络中，上层节点的输出和下层节点的输入之间具有一个函数关系，这个函数称为激活函数（又称激励函数）。
* 用途：引入激活函数是为了增加神经网络模型的非线性。若没有激活函数的每层都相当于矩阵相乘。没有激活函数的神经网络叠加了若干层之后，还是一个线性变换，与单层感知机无异。[参考](https://blog.csdn.net/hy592070616/article/details/120616475)
**1.二分类问题我们一般采用 Sigmoid 函数，  
2.多分类问题我们采用 Softmax 函数，  
3.卷积神经网络的中间层一般会采用 relu 函数**
#### sigmoid激活函数
sigmoid函数也叫Logistic函数，用于隐层神经元输出，取值范围为(0,1)，它可以将一个实数映射到(0,1)的区间，可以用来做二分类。在特征相差比较复杂或是相差不是特别大时效果比较好。
**特点：**  它能够把输入的连续实值变换为0和1之间的输出，特别的，如果是非常大的负数，那么输出就是0；如果是非常大的正数，输出就是1.
然而现在sigmoid函数已经不太受欢迎，实际很少使用了，这是因为它有两个主要缺点：(1)**Sigmoid函数饱和使梯度消失。**(2)**Sigmoid函数的输出不是零中心的**

#### 双曲正切函数(Tanh激活函数)
双曲正切函数是双曲函数的一种。双曲正切函数在数学语言上一般写作tanh。它解决了Sigmoid函数的不以0为中心输出问题，然而，梯度消失的问题和幂运算的问题仍然存在。

#### ReLU
表达式：Relu = max (0, x)

![[ReLU.png]]
**优点**
- 在正区间上解决了梯度消失或爆炸问题
- 计算梯度时只需判断是否大于0，而不用幂函数，计算速度快，收敛速度快

**缺点**
- ReLU的输出非0均值
- 当输入激活函数的值小于0时，对应的神经元的回传梯度为0，导致相应参数永远不会被更新（非正部分不会更新参数）
#### Leaky ReLU
f(x)=max(αx,x),α=0.01
解决了ReLU非正梯度为0，导致有些参数永远不会被更新的问题
#### Maxout激活函数
Maxout函数来源于ICML上的一篇文献《Maxout Networks》，它可以理解为是神经网络中的一层网络，类似于池化层、卷积层一样。我们也可以把Maxout函数看成是网络的激活函数层，我们假设网络某一层的输入特征向量为：x = ( x1 , x2 , ⋯  , xd ) ,也就是我们输入是d 个神经元。Maxout函数的输出如下：Maxout(x)=max(ωi​*xi​+bi​)

**应用中如何选择合适的激活函数？**
凭经验。
1）深度学习往往需要大量时间来处理大量数据，模型的收敛速度是尤为重要的。所以，总体上来讲，训练深度学习网络尽量使用zero-centered数据 (可以经过数据预处理实现) 和zero-centered输出。所以要尽量选择输出具有zero-centered特点的激活函数以加快模型的收敛速度。
2）如果使用 ReLU，那么一定要小心设置 learning rate，而且要注意不要让网络出现很多 “dead” 神经元，如果这个问题不好解决，那么可以试试 Leaky ReLU、PReLU 或者 Maxout.
3）最好不要用 sigmoid，你可以试试 tanh，不过可以预期它的效果会比不上 ReLU 和 Maxout

## 训练
### [反向传播](https://blog.csdn.net/ft_sunshine/article/details/90221691?ops_request_misc=%257B%2522request%255Fid%2522%253A%2522168882241916800185822177%2522%252C%2522scm%2522%253A%252220140713.130102334..%2522%257D&request_id=168882241916800185822177&biz_id=0&utm_medium=distribute.pc_search_result.none-task-blog-2~all~top_positive~default-1-90221691-null-null.142^v88^insert_down28v1,239^v2^insert_chatgpt&utm_term=%E5%8F%8D%E5%90%91%E4%BC%A0%E6%92%AD&spm=1018.2226.3001.4187)
* 定义：**反向传播**（英语：**Backpropagation**，缩写为**BP**）是“**误差反向传播**”的简称，是一种与最优化方法（如梯度下降法）结合使用的，是目前用来训练人工神经网络的最常用且最有效的算法
* 工作原理就是：前向传播、误差反向传播、权值更新
(1)前向传播：将训练集数据输入到ANN的输入层，经过隐藏层，最后到达输出层并输出结果。【输入层—隐藏层–输出层】
(2)反向传播：由于ANN的输入结果与输出结果有误差，则计算估计值与实际值之间的误差，并将该误差从输出层向隐藏层反向传播，直至传播到输入层。【输出层–隐藏层–输入层】
(3)权重更新：在反向传播的过程中，根据误差调整各种参数的

##### 前向传播

**前向（前馈）运算**（激活函数为sigmoid）
![[前向（前馈）运算（激活函数为sigmoid）.png]]

这是具体例子的计算:[反向传播算法实例](https://blog.csdn.net/weixin_42782150/article/details/105378900?ops_request_misc=%257B%2522request%255Fid%2522%253A%2522168886756116800188586764%2522%252C%2522scm%2522%253A%252220140713.130102334..%2522%257D&request_id=168886756116800188586764&biz_id=0&utm_medium=distribute.pc_search_result.none-task-blog-2~all~sobaiduend~default-2-105378900-null-null.142^v88^insert_down28v1,239^v2^insert_chatgpt&utm_term=%E5%8F%8D%E5%90%91%E4%BC%A0%E6%92%AD%E7%AE%97%E6%B3%95%E4%BE%8B%E9%A2%98&spm=1018.2226.3001.4187)(包括了前向传播和反向传播的计算过程)
### 正则化

定义：Regularization，中文翻译过来可以称为**正则化**，或者是**规范化**。什么是规则？闭卷考试中不能查书，这就是规则，一个**限制**。同理，在这里，规则化就是说给损失函数加上一些限制，通过这种规则去规范他们再接下来的循环迭代中，不要自我膨胀。
作用：**正则化存在的意义，能帮助我们在训练模型的过程中，防止模型过拟合**